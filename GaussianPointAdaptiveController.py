import torch
from dataclasses import dataclass


class GaussianPointAdaptiveController:
    @dataclass
    class GaussianPointAdaptiveControllerConfig:
        num_iterations_warm_up: int = 500
        num_iterations_densify: int = 100
        # from paper: densify every 100 iterations and remove any Gaussians that are essentially transparent, i.e., with ğ›¼ less than a threshold ğœ–ğ›¼.
        transparent_alpha_threshold: float = 1.0 / 255.0
        # from paper: densify Gaussians with an average magnitude of view-space position gradients above a threshold ğœpos, which we set to 0.0002 in our tests.
        # I have no idea why their threshold is so low, may be their view space is normalized to [0, 1]?
        # TODO: find out a proper threshold
        densification_view_space_position_gradients_threshold: float = 0.0002 * 1920
        # from paper:  large Gaussians in regions with high variance need to be split into smaller Gaussians. We replace such Gaussians by two new ones, and divide their scale by a factor of ğœ™ = 1.6
        gaussian_split_factor_phi: float = 1.6
        # in paper section 5.2, they describe a method to moderate the increase in the number of Gaussians is to set the ğ›¼ value close to zero every
        # 3000 iterations. I have no idea how it is implemented. I just assume that it is a reset of ğ›¼ to fixed value.
        num_iterations_reset_alpha: int = 3000
        reset_alpha_value: float = 0.1
